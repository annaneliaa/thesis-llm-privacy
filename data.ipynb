{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7662eb26-29e7-4d70-b2d2-3d11d93b544c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PILE_DIR=\"nl-en/\"\n",
    "from transformers import GPT2Tokenizer\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "903d8c1e-1a84-49b2-b049-8b46d4a10921",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# some code to count the max number of tokens in a sentence in the dataset\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "def max_tokens_in_sentence(file_path):\n",
    "    max_tokens = 0\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        i = 0\n",
    "        for line in file:\n",
    "            i +=1\n",
    "            if(i%1000) == 0:\n",
    "                print(i)\n",
    "            if(line):\n",
    "            # Tokenize the sentence\n",
    "                tokens = nltk.word_tokenize(line)\n",
    "                num_tokens = len(tokens)\n",
    "                # Update max_tokens if current sentence has more tokens\n",
    "                if num_tokens > max_tokens:\n",
    "                    max_tokens = num_tokens\n",
    "    \n",
    "    return max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2cff2937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize a sentence and return its length\n",
    "def tokenize_sentence(sentence, tokenizer):\n",
    "    tokens = tokenizer.encode(sentence, max_length=1024, truncation=True)\n",
    "    return len(tokens)\n",
    "\n",
    "# Function to generate the csv byte offset file from the original dataset\n",
    "def generate_dataset(input_file, output_file, tokenizer):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    with open(output_file, \"w\", newline='', encoding=\"utf-8\") as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow([\"exid\", \"fid\", \"line_byte_offset\", \"start\", \"end\", \"take_before\", \"take_after\", \"internal_offset\", \"size\", \"start_byte\", \"end_byte\", \"count\"])\n",
    "        \n",
    "        line_byte_offset = 0\n",
    "        exid = 0\n",
    "        fid = 0\n",
    "        for line in lines:\n",
    "            # Remove leading/trailing whitespaces and newline characters\n",
    "            line = line.strip()\n",
    "            \n",
    "            # Calculate the end position (end of sentence)\n",
    "            end = len(line) - 1\n",
    "\n",
    "            # Tokenize the sentence and get its length\n",
    "            size = tokenize_sentence(line, tokenizer)\n",
    "            \n",
    "            # Write the row to the CSV file\n",
    "            csv_writer.writerow([exid, fid, line_byte_offset, 0, end, 0, 0, 0, size, -1, -1, -1])\n",
    "            \n",
    "            # Update line byte offset for the next sentence\n",
    "            line_byte_offset += len(line) + 1  # Add 1 for the newline character\n",
    "            exid += 1  # Increment example ID for the next sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "88eb080f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset generated successfully!\n"
     ]
    }
   ],
   "source": [
    "path = \"/Users/annavisman/stack/RUG/CS/Year3/thesis/thesis-llm-privacy/nl-en/europarl-v7.nl-en-200.en\"\n",
    "\n",
    "output_file = \"datasets/train_dataset_en_200.csv\"\n",
    "# Initialize the tokenizer\n",
    "# use other one here?\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Generate the dataset\n",
    "generate_dataset(path, output_file, tokenizer)\n",
    "    \n",
    "print(\"Dataset generated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7f912502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_sentence(sentence, max_tokens):\n",
    "    # Tokenize the sentence\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    \n",
    "    # Truncate to max_tokens tokens\n",
    "    truncated_tokens = tokens[:max_tokens]\n",
    "    \n",
    "    # Convert tokens back to string\n",
    "    truncated_sentence = tokenizer.convert_tokens_to_string(truncated_tokens)\n",
    "    \n",
    "    return truncated_sentence\n",
    "\n",
    "def filter_and_truncate_sentences(input_file, output_file, max_tokens):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f_input, \\\n",
    "         open(output_file, \"w\", encoding=\"utf-8\") as f_output:\n",
    "        \n",
    "        for line in f_input:\n",
    "            # Remove leading/trailing whitespaces and newline characters\n",
    "            sentence = line.strip()\n",
    "            \n",
    "            # Tokenize the sentence\n",
    "            tokens = tokenizer.tokenize(sentence)\n",
    "            \n",
    "            # Check if the number of tokens exceeds the maximum\n",
    "            if len(tokens) >= max_tokens:\n",
    "                # Truncate the sentence to max_tokens tokens\n",
    "                truncated_sentence = truncate_sentence(sentence, max_tokens)\n",
    "                \n",
    "                # Write the truncated sentence to the output file\n",
    "                f_output.write(truncated_sentence + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ccff0652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered and truncated lines written to: nl-en/europarl-v7.nl-en-100.en\n"
     ]
    }
   ],
   "source": [
    "# stripping the original dataset to sentences of 200 tokens\n",
    "input = \"nl-en/europarl-v7.nl-en.en\"\n",
    "\n",
    "output = \"nl-en/europarl-v7.nl-en-200.en\"\n",
    "\n",
    "# Filter and truncate lines in the input file\n",
    "filter_and_truncate_sentences(input, output, 200)\n",
    "\n",
    "print(\"Filtered and truncated lines written to:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02a1d437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i want to know how many entries have more than 200 tokens in a sentence\n",
    "\n",
    "def count_large_entries(csv_file, tokens):\n",
    "    # Open the CSV file for reading\n",
    "    with open(csv_file, \"r\", newline='', encoding=\"utf-8\") as csvfile:\n",
    "        csv_reader = csv.DictReader(csvfile)\n",
    "        \n",
    "        # Initialize a counter for large entries\n",
    "        large_entry_count = 0\n",
    "        \n",
    "        # Iterate through each row in the CSV file\n",
    "        for row in csv_reader:\n",
    "            # Convert the value in the \"size\" column to an integer\n",
    "            # this is the number of tokens in the example\n",
    "            size = int(row[\"size\"])\n",
    "            \n",
    "            # Check if the size is greater than or equal to the amount of tokens supplied\n",
    "            if size >= tokens:\n",
    "                # Increment the counter if the condition is met\n",
    "                large_entry_count += 1\n",
    "                \n",
    "    return large_entry_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ba25adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with size >=  200 : 282\n"
     ]
    }
   ],
   "source": [
    "csv_file = \"datasets/train_dataset_en.csv\"\n",
    "tokens = 200\n",
    "\n",
    "# Count the number of large entries\n",
    "count = count_large_entries(csv_file, tokens)\n",
    "    \n",
    "print(\"Number of rows with size >= \", tokens, \":\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70b25fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating a jsonlines version of dataset to work with llm benchmark code\n",
    "\n",
    "def text_to_jsonlines(input_file, output_file):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f_input, \\\n",
    "         open(output_file, \"w\", encoding=\"utf-8\") as f_output:\n",
    "        id = 0\n",
    "\n",
    "        for line in f_input:\n",
    "            # Remove leading/trailing whitespaces and newline characters\n",
    "            line = line.strip()\n",
    "            \n",
    "            # Skip empty lines\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Create a JSON object with a \"text\" field containing the line\n",
    "            json_object = {\"exid\": id,\n",
    "                           \"text\": line}\n",
    "            \n",
    "            # Write the JSON object to the output file as a single line\n",
    "            json.dump(json_object, f_output, ensure_ascii=False)\n",
    "            f_output.write('\\n')\n",
    "            id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e8c4a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion completed successfully!\n"
     ]
    }
   ],
   "source": [
    "input_file = \"nl-en/europarl-v7.nl-en-200.en\"\n",
    "output_file = \"nl-en/europarl-v7.nl-en-200.en.jsonl\"\n",
    "\n",
    "# Convert the plain text dataset to JSON Lines format\n",
    "text_to_jsonlines(input_file, output_file)\n",
    "    \n",
    "print(\"Conversion completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59435747",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/envs/torch/lib/python3.11/logging/__init__.py\", line 1110, in emit\n",
      "    msg = self.format(record)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/torch/lib/python3.11/logging/__init__.py\", line 953, in format\n",
      "    return fmt.format(record)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/torch/lib/python3.11/logging/__init__.py\", line 687, in format\n",
      "    record.message = record.getMessage()\n",
      "                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/torch/lib/python3.11/logging/__init__.py\", line 377, in getMessage\n",
      "    msg = msg % self.args\n",
      "          ~~~~^~~~~~~~~~~\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/Users/annavisman/stack/RUG/CS/Year3/thesis/thesis-llm-privacy/load_dataset.py\", line 54, in <module>\n",
      "    logger.info(\"===== Starting dataset token split generation for language\", LANGUAGE, \"with token length\", EXAMPLE_TOKEN_LEN, \"=====\")\n",
      "Message: '===== Starting dataset token split generation for language'\n",
      "Arguments: ('nl', 'with token length', 200, '=====')\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/annavisman/stack/RUG/CS/Year3/thesis/thesis-llm-privacy/load_dataset.py\", line 54, in <module>\n",
      "    logger.info(\"===== Starting dataset token split generation for language\", LANGUAGE, \"with token length\", EXAMPLE_TOKEN_LEN, \"=====\")\n",
      "  File \"/opt/miniconda3/envs/torch/lib/python3.11/logging/__init__.py\", line 1489, in info\n",
      "    self._log(INFO, msg, args, **kwargs)\n",
      "  File \"/opt/miniconda3/envs/torch/lib/python3.11/logging/__init__.py\", line 1634, in _log\n",
      "    self.handle(record)\n",
      "  File \"/opt/miniconda3/envs/torch/lib/python3.11/logging/__init__.py\", line 1644, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/opt/miniconda3/envs/torch/lib/python3.11/logging/__init__.py\", line 1706, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/opt/miniconda3/envs/torch/lib/python3.11/logging/__init__.py\", line 978, in handle\n",
      "    self.emit(record)\n",
      "  File \"/Users/annavisman/stack/RUG/CS/Year3/thesis/thesis-llm-privacy/load_dataset.py\", line 24, in emit\n",
      "    display(self.format(record))\n",
      "            ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/torch/lib/python3.11/logging/__init__.py\", line 953, in format\n",
      "    return fmt.format(record)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/torch/lib/python3.11/logging/__init__.py\", line 687, in format\n",
      "    record.message = record.getMessage()\n",
      "                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/torch/lib/python3.11/logging/__init__.py\", line 377, in getMessage\n",
      "    msg = msg % self.args\n",
      "          ~~~~^~~~~~~~~~~\n",
      "TypeError: not all arguments converted during string formatting\n"
     ]
    }
   ],
   "source": [
    "# running llm benchmark dataset split code to generate prefixes and suffixes\n",
    "\n",
    "!python load_dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ccc5b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "\n",
    "# convert suffixes to jsonlines format\n",
    "def generations_to_jsonl(output_file_path: str, data: np.ndarray):\n",
    "    \"\"\"Converts the tokenized data to a JSONL file at `path`.\"\"\"\n",
    "\n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\", newline='') as file:\n",
    "        id = 0\n",
    "        for row in data:\n",
    "            # Convert token IDs to strings\n",
    "            # replace token space character with empty string\n",
    "            decoded_string = tokenizer.decode(row, skip_special_tokens=True).replace('Ġ', '').replace(\"Ä\", '')\n",
    "            line = decoded_string.strip()\n",
    "\n",
    "            # Skip empty lines\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            # Create a JSON object with a \"text\" field containing the line\n",
    "            json_object = {\"exid\": id,\n",
    "                           \"text\": line}\n",
    "\n",
    "            # Write the JSON object to the output file as a single line\n",
    "            json.dump(json_object, file, ensure_ascii=False)\n",
    "            file.write(\"\\n\")\n",
    "            id += 1\n",
    "\n",
    "    print(\"Decoded strings saved to: %s\", str(output_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ace29f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded strings saved to: %s datasets/en/200/train_prefix.jsonl\n"
     ]
    }
   ],
   "source": [
    "output_file = \"datasets/en/200/train_prefix.jsonl\"\n",
    "data = np.load(\"datasets/en/200/train_prefix.npy\")\n",
    "\n",
    "generations_to_jsonl(output_file, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b272df67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 200\n",
      "Tokens: [464, 1306, 2378, 318, 262, 12886, 1808, 284, 262, 4513, 357, 33, 20, 12, 15, 22136, 14, 11024, 8, 416, 1770, 39004, 726, 68, 11, 9074, 317, 1434, 72, 11, 1770, 14551, 86, 805, 11, 1770, 9740, 68, 11, 9074, 17419, 2934, 8704, 10788, 567, 305, 11, 9074, 2744, 23454, 11, 1770, 18133, 11243, 14057, 11, 1770, 2129, 21107, 11, 1770, 2935, 1689, 11, 9074, 17109, 6557, 36858, 6184, 223, 6780, 19655, 11, 9074, 13145, 12, 4507, 600, 11, 1770, 28493, 421, 959, 4598, 7778, 4533, 11, 1770, 5966, 694, 258, 263, 11, 9074, 14769, 3937, 64, 11, 1770, 25732, 747, 75, 12, 35, 30570, 69, 1754, 11, 9074, 406, 2013, 368, 1236, 11, 9074, 15257, 11, 9074, 48403, 11, 9074, 6669, 274, 11, 1770, 3981, 8836, 710, 89, 3981, 8836, 710, 89, 11, 1770, 14185, 323, 1236, 27321, 11, 9074, 3208, 5034, 11, 1770, 943, 16921, 16528, 11, 1770, 11634, 861, 64, 11, 1770, 371, 444, 11, 1770, 6882, 11, 1770, 390, 371, 2238, 11, 9074, 3837, 65, 21241, 74, 11, 9074, 47349, 276, 353, 11, 9074, 311, 1211, 8546, 3981, 8836, 710, 89, 11, 1770, 44919, 274, 11, 1770, 18949, 694, 87, 11, 9074, 38859, 18840, 1312, 327]\n",
      "Decoded sentence: The next item is the oral question to the Commission (B5-0206/2000) by Mr Lannoye, Mrs Auroi, Mr Bouwman, Mr Bowe, Mrs Cerdeira Morterero, Mrs Corbey, Mr Costa Paolo, Mr Deprez, Mr Desama, Mrs González Álvarez, Mrs Guy-Quint, Mr Izquierdo Collado, Mr Jonckheer, Mrs Korhola, Mr Kreissl-Dörfler, Mrs Lienemann, Mrs Lucas, Mrs McKenna, Mrs Maes, Mr Martínez Martínez, Mr Papayannakis, Mrs Patrie, Mr Arvidsson, Mr Puerta, Mr Ries, Mr Rod, Mr de Roo, Mrs Sandbæk, Mrs Schroedter, Mrs Sornosa Martínez, Mr Staes, Mr Sterckx, Mrs Terrón i C\n",
      "Decoded preprefix: The next item is the oral question to the Commission (B5-0206/2000) by Mr Lannoye, Mrs Auroi, Mr Bouwman, Mr Bowe, Mrs Cerdeira Morterero, Mrs Corbey, Mr Costa Paolo, Mr Deprez, Mr Desama, Mrs González Álvarez, Mrs Guy-Quint, Mr Izquierdo Collado, Mr Jonckheer, Mrs Korhola, Mr Kre\n",
      "Decoded prefix: issl-Dörfler, Mrs Lienemann, Mrs Lucas, Mrs McKenna, Mrs Maes, Mr Martínez Martínez, Mr Papayannakis, Mrs Patrie, Mr Arvidsson, Mr\n",
      "Decoded suffix:  Puerta, Mr Ries, Mr Rod, Mr de Roo, Mrs Sandbæk, Mrs Schroedter, Mrs Sornosa Martínez, Mr Staes, Mr Sterckx, Mrs Terrón i C\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "\n",
    "json_object = {\"text\": \"The next item is the oral question to the Commission (B5-0206/2000) by Mr Lannoye, Mrs Auroi, Mr Bouwman, Mr Bowe, Mrs Cerdeira Morterero, Mrs Corbey, Mr Costa Paolo, Mr Deprez, Mr Desama, Mrs González Álvarez, Mrs Guy-Quint, Mr Izquierdo Collado, Mr Jonckheer, Mrs Korhola, Mr Kreissl-Dörfler, Mrs Lienemann, Mrs Lucas, Mrs McKenna, Mrs Maes, Mr Martínez Martínez, Mr Papayannakis, Mrs Patrie, Mr Arvidsson, Mr Puerta, Mr Ries, Mr Rod, Mr de Roo, Mrs Sandbæk, Mrs Schroedter, Mrs Sornosa Martínez, Mr Staes, Mr Sterckx, Mrs Terrón i C\"}\n",
    "tokens = tokenizer.encode(json_object[\"text\"], max_length=1024, truncation=True)\n",
    "\n",
    "print(\"Number of tokens:\", len(tokens))\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# split tokens into prefix and suffix, both 50 tokens long\n",
    "prefix_tokens = tokens[100:150]\n",
    "suffix_tokens = tokens[150:200]\n",
    "preprefix_tokens = tokens[:100]\n",
    "\n",
    "sentence = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "print(\"Decoded sentence:\", sentence)\n",
    "\n",
    "preprefix = tokenizer.decode(preprefix_tokens, skip_special_tokens=True)\n",
    "print(\"Decoded preprefix:\", preprefix)\n",
    "\n",
    "prefix = tokenizer.decode(prefix_tokens, skip_special_tokens=True)\n",
    "print(\"Decoded prefix:\", prefix)\n",
    "\n",
    "suffix = tokenizer.decode(suffix_tokens, skip_special_tokens=True)\n",
    "print(\"Decoded suffix:\", suffix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
