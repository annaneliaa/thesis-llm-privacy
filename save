# # modified function for happy transformer trained model
# def generate_for_prompts(
#     prompts: np.ndarray, batch_size: int, suffix_len: int, prefix_len: int
# ) -> Tuple[np.ndarray, np.ndarray]:
#     """Generates suffixes given `prompts` and scores using their likelihood."""
#     generations = []
#     losses = []
#     generation_len = suffix_len + prefix_len
#     args = GENSettings(
#         max_length=generation_len, do_sample=True, top_k=10, top_p=1, pad_token_id=50256
#     )

#     for i, off in enumerate(range(0, len(prompts), batch_size)):
#         prompt_batch = prompts[off : off + batch_size]
#         logger.info(f"Generating for batch ID {i:05} of size {len(prompt_batch):04}")
#         prompt_batch = np.stack(prompt_batch, axis=0)
#         input_ids = torch.tensor(prompt_batch, dtype=torch.int64).to(DEFAULT_DEVICE)
#         input_ids.to(DEFAULT_DEVICE),
#         with torch.no_grad():
#             generated_tokens = MODEL.generate(input_ids, args=args).to("cpu").detach()
#             outputs = MODEL(
#                 generated_tokens.to(DEFAULT_DEVICE),
#                 labels=generated_tokens.to(DEFAULT_DEVICE),
#             )
#             logits = outputs.logits.cpu().detach()
#             logits = logits[:, :-1].reshape((-1, logits.shape[-1])).float()
#             loss_per_token = torch.nn.functional.cross_entropy(
#                 logits, generated_tokens[:, 1:].flatten(), reduction="none"
#             )
#             loss_per_token = loss_per_token.reshape((-1, generation_len - 1))[
#                 :, -suffix_len - 1 : -1
#             ]
#             likelihood = loss_per_token.mean(1)
#             generations.extend(generated_tokens.numpy())
#             losses.extend(likelihood.numpy())
#     return np.atleast_2d(generations), np.atleast_2d(losses).reshape(
#         (len(generations), -1)
#     )