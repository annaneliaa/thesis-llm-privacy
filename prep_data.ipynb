{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset constants\n",
    "\n",
    "# path to the (pretraining) dataset of the model\n",
    "DATASET_DIR = \"ECB\"\n",
    "# file name of text version of the dataset\n",
    "# add a \"-c\" when running processing script after preprocessing (if dataset needs to be concatenated)\n",
    "DATASET_NAME = \"ECB-c\"\n",
    "# language of the setup\n",
    "LANGUAGE = \"en\"\n",
    "# target directory for the csv files\n",
    "SOURCE_DIR = \"./datasets\"\n",
    "# desired token length of examples\n",
    "EXAMPLE_TOKEN_LEN = 100\n",
    "# target file name for the byte off set csv files\n",
    "\n",
    "# create data_config.json from constants\n",
    "config = {\n",
    "    \"dataset_dir\": DATASET_DIR,\n",
    "    \"dataset_name\": DATASET_NAME,\n",
    "    \"language\": LANGUAGE,\n",
    "    \"example_token_len\": EXAMPLE_TOKEN_LEN,\n",
    "    \"source_dir\": SOURCE_DIR,\n",
    "}\n",
    "\n",
    "with open(\"data_config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-07 14:38:46,838 - INFO - Parsing arguments...\n",
      "Parsing arguments...\n",
      "2024-06-07 14:38:46,839 - INFO - Loading tokenizer...\n",
      "Loading tokenizer...\n",
      "2024-06-07 14:38:47,065 - INFO - ==== Starting data preprocessing script ====\n",
      "==== Starting data preprocessing script ====\n",
      "2024-06-07 14:38:47,065 - INFO - This may take a while depending on the size of the dataset...\n",
      "This may take a while depending on the size of the dataset...\n",
      "2024-06-07 14:38:47,065 - INFO - Counting tokens for en...\n",
      "Counting tokens for en...\n",
      "Generating byte offset dataset from file:  ECB/ECB.en\n",
      "2024-06-07 14:38:55,945 - INFO - Number of samples >= 100 tokens in ./datasets/ECB/csv/ECB.en.csv: 2441\n",
      "Number of samples >= 100 tokens in ./datasets/ECB/csv/ECB.en.csv: 2441\n",
      "2024-06-07 14:38:55,945 - INFO - Counting tokens for nl...\n",
      "Counting tokens for nl...\n",
      "Generating byte offset dataset from file:  ECB/ECB.nl\n",
      "2024-06-07 14:39:09,131 - INFO - Number of samples >= 100 tokens in ./datasets/ECB/csv/ECB.nl.csv: 19620\n",
      "Number of samples >= 100 tokens in ./datasets/ECB/csv/ECB.nl.csv: 19620\n",
      "2024-06-07 14:39:09,620 - INFO - Concatenated sentences in en to reach 8200 samples >= 100 tokens\n",
      "Concatenated sentences in en to reach 8200 samples >= 100 tokens\n",
      "2024-06-07 14:39:09,620 - INFO - Generating JSONL for both languages...\n",
      "Generating JSONL for both languages...\n",
      "2024-06-07 14:39:09,952 - INFO - ==== Data preprocessing complete ====\n",
      "==== Data preprocessing complete ====\n"
     ]
    }
   ],
   "source": [
    "!python preprocessing.py --config_file data_config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-07 14:39:29,920 - INFO - Parsing arguments...\n",
      "Parsing arguments...\n",
      "2024-06-07 14:39:29,920 - INFO - Loading tokenizer...\n",
      "Loading tokenizer...\n",
      "2024-06-07 14:39:30,205 - INFO - ==== Sarting data processing script ====\n",
      "==== Sarting data processing script ====\n",
      "2024-06-07 14:39:30,205 - INFO - This may take a while depending on the size of the dataset...\n",
      "This may take a while depending on the size of the dataset...\n",
      "2024-06-07 14:39:30,205 - INFO - Counting tokens for en...\n",
      "Counting tokens for en...\n",
      "Generating byte offset dataset from file:  ECB/ECB-c.en\n",
      "2024-06-07 14:39:38,244 - INFO - Number of samples >= 100 tokens in ./datasets/ECB/csv/ECB-c.en.csv: 8170\n",
      "Number of samples >= 100 tokens in ./datasets/ECB/csv/ECB-c.en.csv: 8170\n",
      "2024-06-07 14:39:38,244 - INFO - Filtering sentences for en...\n",
      "Filtering sentences for en...\n",
      "2024-06-07 14:39:38,297 - INFO - Generating JSONL for en...\n",
      "Generating JSONL for en...\n",
      "2024-06-07 14:39:38,706 - INFO - Counting tokens for nl...\n",
      "Counting tokens for nl...\n",
      "Generating byte offset dataset from file:  ECB/ECB-c.nl\n",
      "2024-06-07 14:39:50,246 - INFO - Number of samples >= 100 tokens in ./datasets/ECB/csv/ECB-c.nl.csv: 34865\n",
      "Number of samples >= 100 tokens in ./datasets/ECB/csv/ECB-c.nl.csv: 34865\n",
      "2024-06-07 14:39:50,246 - INFO - Filtering sentences for nl...\n",
      "Filtering sentences for nl...\n",
      "2024-06-07 14:39:50,317 - INFO - Generating JSONL for nl...\n",
      "Generating JSONL for nl...\n",
      "Number of exids in file 1: %s 8170\n",
      "Number of exids in file 2: %s 34865\n",
      "Number of common exids found %s 5066\n",
      "2024-06-07 14:39:50,798 - INFO - Common exids have been written to ./datasets/ECB/csv/common_exids-100.csv\n",
      "Common exids have been written to ./datasets/ECB/csv/common_exids-100.csv\n",
      "2024-06-07 14:39:50,798 - INFO - 5066 common example IDs found\n",
      "5066 common example IDs found\n",
      "Truncating sentences in file:  ECB/ECB-c.en.jsonl  to  100  tokens\n",
      "Truncated  5066  sentences to  ECB/ECB-c-100.en.jsonl\n",
      "Done!\n",
      "Truncating sentences in file:  ECB/ECB-c.nl.jsonl  to  100  tokens\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1243 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Truncated  5066  sentences to  ECB/ECB-c-100.nl.jsonl\n",
      "Done!\n",
      "2024-06-07 14:39:59,249 - INFO - ==== Data processing script completed ====\n",
      "==== Data processing script completed ====\n"
     ]
    }
   ],
   "source": [
    "!python process_data.py --config_file data_config.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
