{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from transformers import GPT2Tokenizer\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define dataset constants\n",
    "\n",
    "# path to the (pretraining) dataset of the model\n",
    "DATASET_DIR = \"nl-en/\"\n",
    "# file name of text version of the dataset\n",
    "DATASET_NAME = \"europarl-v7.nl-en\"\n",
    "# language of the setup\n",
    "LANGUAGE = \"nl\"\n",
    "# target directory for the csv files\n",
    "TARGET_DIR = \"./datasets/\" + \"csv/\"\n",
    "# desired token length of examples\n",
    "TOKEN_LENGTH = 100\n",
    "# target file name for the byte off set csv files\n",
    "BYTE_OFFSET_FILE = DATASET_NAME + \".\" + LANGUAGE + \".csv\"\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "byte_offset_base = os.path.join(TARGET_DIR, BYTE_OFFSET_FILE)\n",
    "\n",
    "# create data_config.json from constants\n",
    "config = {\n",
    "    \"dataset_dir\": DATASET_DIR,\n",
    "    \"dataset_name\": DATASET_NAME,\n",
    "    \"language\": LANGUAGE,\n",
    "    \"token_length\": TOKEN_LENGTH,\n",
    "    \"target_dir\": TARGET_DIR,\n",
    "    \"byte_offset_file\": BYTE_OFFSET_FILE\n",
    "}\n",
    "\n",
    "with open(\"data_config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data set inspection functions\n",
    "\n",
    "# function to count the longest token sequence in a dataset file \n",
    "def max_tokens_in_sentence(file_path):\n",
    "    max_tokens = 0\n",
    "\n",
    "    print(\"Counting max tokens in file: \", file_path)\n",
    "    print(\"This may take a while...\")\n",
    "    with open(file_path, 'r') as file:\n",
    "        i = 0\n",
    "        for line in file:\n",
    "            i +=1\n",
    "            if(line):\n",
    "            # Tokenize the sentence\n",
    "                tokens = nltk.word_tokenize(line)\n",
    "                num_tokens = len(tokens)\n",
    "                # Update max_tokens if current sentence has more tokens\n",
    "                if num_tokens > max_tokens:\n",
    "                    max_tokens = num_tokens\n",
    "    \n",
    "        print(\"Max tokens in file: \", max_tokens)\n",
    "    return max_tokens\n",
    "\n",
    "# function to count the number of examples in a dataset file with more tokens than a given threshold\n",
    "def count_large_entries(csv_file, tokens):\n",
    "    # Open the CSV file for reading\n",
    "    with open(csv_file, \"r\", newline='', encoding=\"utf-8\") as csvfile:\n",
    "        csv_reader = csv.DictReader(csvfile)\n",
    "        \n",
    "        # Initialize a counter for large entries\n",
    "        large_entry_count = 0\n",
    "        \n",
    "        # Iterate through each row in the CSV file\n",
    "        for row in csv_reader:\n",
    "            # Convert the value in the \"size\" column to an integer\n",
    "            # this is the number of tokens in the example\n",
    "            size = int(row[\"size\"])\n",
    "            \n",
    "            # Check if the size is greater than or equal to the amount of tokens supplied\n",
    "            if size >= tokens:\n",
    "                # Increment the counter if the condition is met\n",
    "                large_entry_count += 1\n",
    "                \n",
    "    return large_entry_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset generation functions\n",
    "\n",
    "def truncate_sentence(sentence, max_tokens):\n",
    "    # Tokenize the sentence\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    \n",
    "    # Truncate to max_tokens tokens\n",
    "    truncated_tokens = tokens[:max_tokens]\n",
    "    \n",
    "    # Convert tokens back to string\n",
    "    truncated_sentence = tokenizer.convert_tokens_to_string(truncated_tokens)\n",
    "    \n",
    "    return truncated_sentence\n",
    "\n",
    "def truncate_tokens(tokens, max_tokens):\n",
    "    # Truncate to max_tokens tokens\n",
    "    truncated_tokens = tokens[:max_tokens]\n",
    "    \n",
    "    # Convert tokens back to string\n",
    "    truncated_sentence = tokenizer.convert_tokens_to_string(truncated_tokens)\n",
    "    \n",
    "    return truncated_sentence\n",
    "\n",
    "def filter_truncate_json_sentences(input_file, output_file, max_tokens):\n",
    "    print(\"Filtering and truncating sentences in file: \", input_file, \" to \", max_tokens, \" tokens\")\n",
    "    \n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f_input, \\\n",
    "         open(output_file, \"w\", encoding=\"utf-8\") as f_output:\n",
    "        \n",
    "        for line in f_input:\n",
    "            json_object = json.loads(line)\n",
    "\n",
    "            sentence = json_object[\"text\"]\n",
    "\n",
    "            # Skip empty lines\n",
    "            if not sentence:\n",
    "                continue\n",
    "\n",
    "            exid = json_object[\"exid\"]\n",
    "            # Remove leading/trailing whitespaces and newline characters\n",
    "            sentence = sentence.strip()\n",
    "            \n",
    "            # Tokenize the sentence\n",
    "            tokens = tokenizer.tokenize(sentence)\n",
    "            \n",
    "            # Check if the number of tokens exceeds the maximum\n",
    "            if len(tokens) >= max_tokens:\n",
    "\n",
    "                # Truncate the tokenized sentece to max amount of tokens\n",
    "                truncated_sentence = truncate_tokens(tokens, max_tokens)\n",
    "\n",
    "                # Create a JSON object with a \"text\" field containing the line\n",
    "                # and the original example ID\n",
    "                trunc_object = {\"exid\": exid,\n",
    "                               \"text\": truncated_sentence}\n",
    "                \n",
    "                # Write the JSON object to the output file as a single line\n",
    "                json.dump(trunc_object, f_output, ensure_ascii=False)\n",
    "                f_output.write('\\n')\n",
    "\n",
    "def filter_and_truncate_sentences(input_file, output_file, max_tokens):\n",
    "    print(\"Filtering and truncating sentences in file: \", input_file, \" to \", max_tokens, \" tokens\")\n",
    "    \n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f_input, \\\n",
    "         open(output_file, \"w\", encoding=\"utf-8\") as f_output:\n",
    "        \n",
    "        for line in f_input:\n",
    "            # Remove leading/trailing whitespaces and newline characters\n",
    "            sentence = line.strip()\n",
    "            \n",
    "            # Tokenize the sentence\n",
    "            tokens = tokenizer.tokenize(sentence)\n",
    "            \n",
    "            # Check if the number of tokens exceeds the maximum\n",
    "            if len(tokens) >= max_tokens:\n",
    "\n",
    "                # Truncate the tokenized sentece to max amount of tokens\n",
    "                truncated_sentence = truncate_tokens(tokens, max_tokens)\n",
    "                \n",
    "                # Write the truncated sentence to the output file\n",
    "                f_output.write(truncated_sentence + \"\\n\")\n",
    "\n",
    "# Function to tokenize a sentence and return its length\n",
    "def tokenize_sentence(sentence, tokenizer):\n",
    "    tokens = tokenizer.encode(sentence, max_length=1024, truncation=True)\n",
    "    return len(tokens)\n",
    "\n",
    "# Function to generate a csv byte offset file from the original dataset\n",
    "# used to work with Carlini code only\n",
    "def generate_byte_dataset(input_file, output_file, tokenizer):\n",
    "    print(\"Generating byte offset dataset from file: \", input_file)\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    if not os.path.exists(TARGET_DIR):\n",
    "        os.makedirs(TARGET_DIR)\n",
    "        \n",
    "    with open(output_file, \"w\", newline='', encoding=\"utf-8\") as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        #csv_writer.writerow([\"exid\", \"fid\", \"line_byte_offset\", \"start\", \"end\", \"take_before\", \"take_after\", \"internal_offset\", \"size\", \"start_byte\", \"end_byte\", \"count\"])\n",
    "        csv_writer.writerow([\"exid\", \"size\"])\n",
    "        \n",
    "        exid = 1 # start at 1\n",
    "\n",
    "        #line_byte_offset = 0\n",
    "        #fid = 0\n",
    "        for line in lines:\n",
    "            # Remove leading/trailing whitespaces and newline characters\n",
    "            line = line.strip()\n",
    "\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            else: \n",
    "                # Calculate the end position (end of sentence)\n",
    "                #end = len(line) - 1\n",
    "\n",
    "                # Tokenize the sentence and get its length\n",
    "                size = len(tokenizer.encode(line, truncation=True))\n",
    "                \n",
    "                # Write the row to the CSV file\n",
    "                #csv_writer.writerow([exid, fid, line_byte_offset, 0, end, 0, 0, 0, size, -1, -1, -1])\n",
    "                csv_writer.writerow([exid, size])\n",
    "                # Update line byte offset for the next sentence\n",
    "                #line_byte_offset += len(line) + 1  # Add 1 for the newline character\n",
    "            \n",
    "            exid += 1  # Always increment the example ID to keep in sync with original dataset\n",
    "\n",
    "\n",
    "# Function to generate a jsonlines version of dataset\n",
    "# input here is a text file\n",
    "def text_to_jsonlines(input_file, output_file):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f_input, \\\n",
    "         open(output_file, \"w\", encoding=\"utf-8\") as f_output:\n",
    "        id = 0\n",
    "\n",
    "        for line in f_input:\n",
    "            # Remove leading/trailing whitespaces and newline characters\n",
    "            line = line.strip()\n",
    "            \n",
    "            # Skip empty lines\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Create a JSON object with a \"text\" field containing the line\n",
    "            json_object = {\"exid\": id,\n",
    "                           \"text\": line}\n",
    "            \n",
    "            # Write the JSON object to the output file as a single line\n",
    "            json.dump(json_object, f_output, ensure_ascii=False)\n",
    "            f_output.write('\\n')\n",
    "            id += 1\n",
    "\n",
    "# Function to generate a jsonlines version of dataset\n",
    "# input here is a numpy array of tokenized data (using token IDs)\n",
    "def generations_to_jsonl(output_file_path: str, data: np.ndarray):\n",
    "    \"\"\"Converts the tokenized data to a JSONL file at `path`.\"\"\"\n",
    "\n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\", newline='') as file:\n",
    "        id = 0\n",
    "        for row in data:\n",
    "            # Convert token IDs to strings\n",
    "            # replace token space character with empty string\n",
    "            decoded_string = tokenizer.decode(row, skip_special_tokens=True).replace('Ä ', '')\n",
    "            line = decoded_string.strip()\n",
    "\n",
    "            # Skip empty lines\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            # Create a JSON object with a \"text\" field containing the line\n",
    "            json_object = {\"exid\": id,\n",
    "                           \"text\": line}\n",
    "\n",
    "            # Write the JSON object to the output file as a single line\n",
    "            json.dump(json_object, file, ensure_ascii=False)\n",
    "            file.write(\"\\n\")\n",
    "            id += 1\n",
    "\n",
    "    print(\"Decoded strings saved to: %s\", str(output_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operating on dataset base: nl-en/europarl-v7.nl-en in language en\n",
      "Dataset file: nl-en/europarl-v7.nl-en.en\n",
      "Byte offset base: ./datasets/csv/europarl-v7.nl-en.en.csv\n"
     ]
    }
   ],
   "source": [
    "# 1. read data_config.json\n",
    "\n",
    "with open(\"data_config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "    dataset_base = os.path.join(config[\"dataset_dir\"], config[\"dataset_name\"])\n",
    "    dataset_file = os.path.join(dataset_base + \".\" + config[\"language\"])\n",
    "    \n",
    "print(\"Operating on dataset base:\", dataset_base, \"in language\", config[\"language\"])\n",
    "print(\"Dataset file:\", dataset_file)\n",
    "print(\"Byte offset base:\", byte_offset_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating byte offset dataset from file:  nl-en/europarl-v7.nl-en.en\n"
     ]
    }
   ],
   "source": [
    "# 2. Generate a byte offset version of the dataset for inspection purposes\n",
    "generate_byte_dataset(dataset_file, byte_offset_base, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered rows have been written to ./datasets/csv/europarl-v7.nl-en-100.nl.csv\n"
     ]
    }
   ],
   "source": [
    "def filter_csv(input_file, output_file, min_size):\n",
    "    # Open the input CSV file for reading\n",
    "    with open(input_file, mode='r', newline='', encoding='utf-8') as infile:\n",
    "        # Create a CSV reader object\n",
    "        reader = csv.DictReader(infile)\n",
    "        \n",
    "        # Open the output CSV file for writing\n",
    "        with open(output_file, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "            # Create a CSV writer object\n",
    "            writer = csv.DictWriter(outfile, fieldnames=reader.fieldnames)\n",
    "            \n",
    "            # Write the header to the output file\n",
    "            writer.writeheader()\n",
    "            \n",
    "            # Iterate through each row in the input file\n",
    "            for row in reader:\n",
    "                # Check if the size column value is at least min_size\n",
    "                if int(row['size']) >= min_size:\n",
    "                    # Write the row to the output file\n",
    "                    writer.writerow(row)\n",
    "\n",
    "# Input and output file paths\n",
    "input_csv = os.path.join(TARGET_DIR, BYTE_OFFSET_FILE)\n",
    "output_csv = os.path.join(TARGET_DIR, DATASET_NAME + \"-\" + str(TOKEN_LENGTH) + \".\" + LANGUAGE + \".csv\")\n",
    "\n",
    "# Call the function to filter the CSV file\n",
    "filter_csv(input_csv, output_csv, TOKEN_LENGTH)\n",
    "\n",
    "print(f\"Filtered rows have been written to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7591\n",
      "236565\n",
      "943\n",
      "Common exids have been written to datasets/csv/common_exids.csv\n"
     ]
    }
   ],
   "source": [
    "def read_exids_from_csv(file):\n",
    "    # integer set\n",
    "    exids = set()\n",
    "    with open(file, mode='r', newline='', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        next(reader, None)  # Skip the header\n",
    "        for row in reader:\n",
    "            exids.add(int(row['exid']))\n",
    "    return exids, len(exids)\n",
    "\n",
    "def find_common_exids(file1, file2):\n",
    "    exids1, len1 = read_exids_from_csv(file1)\n",
    "    print(len1)\n",
    "    exids2, len2 = read_exids_from_csv(file2)\n",
    "    print(len2)\n",
    "    common_exids = exids1.intersection(exids2)\n",
    "    # sort\n",
    "    common_exids = sorted(common_exids)\n",
    "\n",
    "    print(len(common_exids))\n",
    "    return common_exids\n",
    "\n",
    "def write_exids_to_file(exids, output_file):\n",
    "    with open(output_file, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        for exid in exids:\n",
    "            writer.writerow([exid])\n",
    "\n",
    "def read_common_exids(file):\n",
    "    exids = []\n",
    "    with open(file, mode='r', newline='', encoding='utf-8') as infile:\n",
    "        reader = csv.reader(infile)\n",
    "        for row in reader:\n",
    "            exid = row  # Strip any leading/trailing whitespace\n",
    "            exid = exid[0]\n",
    "            exids.append(exid)\n",
    "    return exids\n",
    "\n",
    "# Input CSV file paths\n",
    "csv_file1 = 'datasets/csv/europarl-v7.nl-en-100.en.csv'\n",
    "csv_file2 = 'datasets/csv/europarl-v7.nl-en-100.nl.csv'\n",
    "output_csv = 'datasets/csv/common_exids.csv'\n",
    "\n",
    "# Find common exids and write them to the output file\n",
    "common_exids = find_common_exids(csv_file1, csv_file2)\n",
    "write_exids_to_file(common_exids, output_csv)\n",
    "\n",
    "print(f\"Common exids have been written to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7398\n",
      "['717815', '1547840', '1896028', '586351', '82', '1649358', '1297059', '1779301', '197955', '852740']\n",
      "Truncating sentences in file:  nl-en/europarl-v7.nl-en.en.jsonl  to  100  tokens\n",
      "Truncated  7341  sentences to  nl-en/europarl-v7.nl-en-100.en.jsonl\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "def trunc_json(input_file, output_file, max_tokens, exid_list):\n",
    "    print(\"Truncating sentences in file: \", input_file, \" to \", max_tokens, \" tokens\")\n",
    "    count = 0\n",
    "    \n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f_input, \\\n",
    "         open(output_file, \"w\", encoding=\"utf-8\") as f_output:\n",
    "        \n",
    "        for line in f_input:\n",
    "            # Remove leading/trailing whitespaces and newline characters\n",
    "            json_object = json.loads(line)\n",
    "            \n",
    "            exid = json_object[\"exid\"]\n",
    "            \n",
    "\n",
    "            if(str(exid) not in exid_list):\n",
    "                continue\n",
    "\n",
    "            else: \n",
    "                sentence = json_object[\"text\"]\n",
    "                # Tokenize the sentence\n",
    "                tokens = tokenizer.tokenize(sentence)\n",
    "            \n",
    "                # Truncate the tokenized sentece to max amount of tokens\n",
    "                truncated_sentence = truncate_tokens(tokens, max_tokens)\n",
    "\n",
    "                trunc_obj = {\"exid\": exid,\n",
    "                             \"text\": truncated_sentence}\n",
    "                    \n",
    "                # Write the truncated sentence to the output file\n",
    "                json.dump(trunc_obj, f_output, ensure_ascii=False)\n",
    "                f_output.write('\\n')\n",
    "                count += 1\n",
    "    print(\"Truncated \", count, \" sentences to \", output_file)\n",
    "    print(\"Done!\")\n",
    "\n",
    "exid_list = read_exids_from_csv('datasets/csv/common_exids_100t.csv')\n",
    "print(len(exid_list))\n",
    "\n",
    "print(exid_list[:10])\n",
    "\n",
    "input_file = 'nl-en/europarl-v7.nl-en.en.jsonl'\n",
    "output_file = 'nl-en/europarl-v7.nl-en-100.en.jsonl'\n",
    "\n",
    "trunc_json(input_file, output_file, TOKEN_LENGTH, exid_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONL file nl-en/europarl-v7.nl-en.en.jsonl already exists, skipping generation\n"
     ]
    }
   ],
   "source": [
    "# Generate a jsonlines version of the dataset\n",
    "jsonlines_base = os.path.join(DATASET_DIR, DATASET_NAME + \".\" + LANGUAGE + \".jsonl\")\n",
    "\n",
    "# check if file exists and has content\n",
    "if os.path.exists(jsonlines_base) and os.path.getsize(jsonlines_base) > 0:\n",
    "    print(\"JSONL file\", jsonlines_base, \"already exists, skipping generation\")\n",
    "else: \n",
    "    text_to_jsonlines(dataset_file, jsonlines_base)\n",
    "    print(\"JSONL file saved to: \", jsonlines_base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncating sentences to  100  tokens and saving to  nl-en/europarl-v7.nl-en-100.en.jsonl\n",
      "Filtering and truncating sentences in file:  nl-en/europarl-v7.nl-en.en.jsonl  to  100  tokens\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[180], line 12\u001b[0m\n\u001b[1;32m      7\u001b[0m output_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dataset_base \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_length\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m+\u001b[39m  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTruncating sentences to \u001b[39m\u001b[38;5;124m\"\u001b[39m, TOKEN_LENGTH, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m tokens and saving to \u001b[39m\u001b[38;5;124m\"\u001b[39m, output_file)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mfilter_truncate_json_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTOKEN_LENGTH\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[62], line 44\u001b[0m, in \u001b[0;36mfilter_truncate_json_sentences\u001b[0;34m(input_file, output_file, max_tokens)\u001b[0m\n\u001b[1;32m     41\u001b[0m sentence \u001b[38;5;241m=\u001b[39m sentence\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Tokenize the sentence\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Check if the number of tokens exceeds the maximum\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tokens) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m max_tokens:\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# Truncate the tokenized sentece to max amount of tokens\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/torch/lib/python3.11/site-packages/transformers/tokenization_utils.py:581\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    579\u001b[0m     no_split_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_added_tokens_encoder\u001b[38;5;241m.\u001b[39mkeys()  \u001b[38;5;66;03m# don't split on any of the added tokens\u001b[39;00m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;66;03m# \"This is something<special_token_1>  else\"\u001b[39;00m\n\u001b[0;32m--> 581\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokens_trie\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;66;03m# [\"This is something\", \"<special_token_1>\", \"  else\"]\u001b[39;00m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tokens):\n",
      "File \u001b[0;32m/opt/miniconda3/envs/torch/lib/python3.11/site-packages/transformers/tokenization_utils.py:148\u001b[0m, in \u001b[0;36mTrie.split\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# This will track every state\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# that stop matching, we need to stop tracking them.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# If we look at \"lowball\", we're going to match \"l\" (add it to states), \"o\", \"w\", then\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# fail on \"b\", we need to remove 0 from the valid states.\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m to_remove \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# Whenever we found a match, we need to drop everything\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# this is a greedy algorithm, it will match on the first found token\u001b[39;00m\n\u001b[1;32m    151\u001b[0m reset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 4. Truncate the sentences in the dataset to the desired token length\n",
    "\n",
    "# input file is the jsonl version of the dataset\n",
    "dataset_file = os.path.join(dataset_base + \".\" + config[\"language\"] + \".jsonl\")\n",
    "\n",
    "# output file is another jsonl file with the truncated sentences, where the original example id is preserved\n",
    "output_file = os.path.join(dataset_base + \"-\" + str(config[\"token_length\"]) +  \".\" + config[\"language\"] + \".jsonl\")\n",
    "\n",
    "\n",
    "print(\"Truncating sentences to \", TOKEN_LENGTH, \" tokens and saving to \", output_file)\n",
    "\n",
    "filter_truncate_json_sentences(dataset_file, output_file, TOKEN_LENGTH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
