{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from transformers import GPT2Tokenizer\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset constants\n",
    "\n",
    "# path to the (pretraining) dataset of the model\n",
    "DATASET_DIR = \"EMEA/\"\n",
    "# file name of text version of the dataset\n",
    "DATASET_NAME = \"EMEA\"\n",
    "# language of the setup\n",
    "LANGUAGE = \"nl\"\n",
    "# target directory for the csv files\n",
    "TARGET_DIR = \"./datasets/\" + DATASET_DIR + \"csv\"\n",
    "# desired token length of examples\n",
    "TOKEN_LENGTH = 100\n",
    "# target file name for the byte off set csv files\n",
    "BYTE_OFFSET_FILE = DATASET_NAME + \".\" + LANGUAGE + \".csv\"\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "byte_offset_base = os.path.join(TARGET_DIR, BYTE_OFFSET_FILE)\n",
    "\n",
    "# create data_config.json from constants\n",
    "config = {\n",
    "    \"dataset_dir\": DATASET_DIR,\n",
    "    \"dataset_name\": DATASET_NAME,\n",
    "    \"language\": LANGUAGE,\n",
    "    \"token_length\": TOKEN_LENGTH,\n",
    "    \"target_dir\": TARGET_DIR,\n",
    "    \"byte_offset_file\": BYTE_OFFSET_FILE\n",
    "}\n",
    "\n",
    "with open(\"data_config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data set inspection functions\n",
    "\n",
    "# function to count the longest token sequence in a dataset file \n",
    "def max_tokens_in_sentence(file_path):\n",
    "    max_tokens = 0\n",
    "\n",
    "    print(\"Counting max tokens in file: \", file_path)\n",
    "    print(\"This may take a while...\")\n",
    "    with open(file_path, 'r') as file:\n",
    "        i = 0\n",
    "        for line in file:\n",
    "            i +=1\n",
    "            if(line):\n",
    "            # Tokenize the sentence\n",
    "                tokens = nltk.word_tokenize(line)\n",
    "                num_tokens = len(tokens)\n",
    "                # Update max_tokens if current sentence has more tokens\n",
    "                if num_tokens > max_tokens:\n",
    "                    max_tokens = num_tokens\n",
    "    \n",
    "        print(\"Max tokens in file: \", max_tokens)\n",
    "    return max_tokens\n",
    "\n",
    "# function to count the number of examples in a dataset file with more tokens than a given threshold\n",
    "def count_large_entries(csv_file, tokens):\n",
    "    # Open the CSV file for reading\n",
    "    with open(csv_file, \"r\", newline='', encoding=\"utf-8\") as csvfile:\n",
    "        csv_reader = csv.DictReader(csvfile)\n",
    "        \n",
    "        # Initialize a counter for large entries\n",
    "        large_entry_count = 0\n",
    "        \n",
    "        # Iterate through each row in the CSV file\n",
    "        for row in csv_reader:\n",
    "            # Convert the value in the \"size\" column to an integer\n",
    "            # this is the number of tokens in the example\n",
    "            size = int(row[\"size\"])\n",
    "            \n",
    "            # Check if the size is greater than or equal to the amount of tokens supplied\n",
    "            if size >= tokens:\n",
    "                # Increment the counter if the condition is met\n",
    "                large_entry_count += 1\n",
    "                \n",
    "    return large_entry_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset generation functions\n",
    "\n",
    "def truncate_sentence(sentence, max_tokens):\n",
    "    # Tokenize the sentence\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    \n",
    "    # Truncate to max_tokens tokens\n",
    "    truncated_tokens = tokens[:max_tokens]\n",
    "    \n",
    "    # Convert tokens back to string\n",
    "    truncated_sentence = tokenizer.convert_tokens_to_string(truncated_tokens)\n",
    "    \n",
    "    return truncated_sentence\n",
    "\n",
    "def truncate_tokens(tokens, max_tokens):\n",
    "    # Truncate to max_tokens tokens\n",
    "    truncated_tokens = tokens[:max_tokens]\n",
    "    \n",
    "    # Convert tokens back to string\n",
    "    truncated_sentence = tokenizer.convert_tokens_to_string(truncated_tokens)\n",
    "    \n",
    "    return truncated_sentence\n",
    "\n",
    "def filter_truncate_json_sentences(input_file, output_file, max_tokens):\n",
    "    print(\"Filtering and truncating sentences in file: \", input_file, \" to \", max_tokens, \" tokens\")\n",
    "    \n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f_input, \\\n",
    "         open(output_file, \"w\", encoding=\"utf-8\") as f_output:\n",
    "        \n",
    "        for line in f_input:\n",
    "            json_object = json.loads(line)\n",
    "\n",
    "            sentence = json_object[\"text\"]\n",
    "\n",
    "            # Skip empty lines\n",
    "            if not sentence:\n",
    "                continue\n",
    "\n",
    "            exid = json_object[\"exid\"]\n",
    "            # Remove leading/trailing whitespaces and newline characters\n",
    "            sentence = sentence.strip()\n",
    "            \n",
    "            # Tokenize the sentence\n",
    "            tokens = tokenizer.tokenize(sentence)\n",
    "            \n",
    "            # Check if the number of tokens exceeds the maximum\n",
    "            if len(tokens) >= max_tokens:\n",
    "\n",
    "                # Truncate the tokenized sentece to max amount of tokens\n",
    "                truncated_sentence = truncate_tokens(tokens, max_tokens)\n",
    "\n",
    "                # Create a JSON object with a \"text\" field containing the line\n",
    "                # and the original example ID\n",
    "                trunc_object = {\"exid\": exid,\n",
    "                               \"text\": truncated_sentence}\n",
    "                \n",
    "                # Write the JSON object to the output file as a single line\n",
    "                json.dump(trunc_object, f_output, ensure_ascii=False)\n",
    "                f_output.write('\\n')\n",
    "\n",
    "def filter_and_truncate_sentences(input_file, output_file, max_tokens):\n",
    "    print(\"Filtering and truncating sentences in file: \", input_file, \" to \", max_tokens, \" tokens\")\n",
    "    \n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f_input, \\\n",
    "         open(output_file, \"w\", encoding=\"utf-8\") as f_output:\n",
    "        \n",
    "        for line in f_input:\n",
    "            # Remove leading/trailing whitespaces and newline characters\n",
    "            sentence = line.strip()\n",
    "            \n",
    "            # Tokenize the sentence\n",
    "            tokens = tokenizer.tokenize(sentence)\n",
    "            \n",
    "            # Check if the number of tokens exceeds the maximum\n",
    "            if len(tokens) >= max_tokens:\n",
    "\n",
    "                # Truncate the tokenized sentece to max amount of tokens\n",
    "                truncated_sentence = truncate_tokens(tokens, max_tokens)\n",
    "                \n",
    "                # Write the truncated sentence to the output file\n",
    "                f_output.write(truncated_sentence + \"\\n\")\n",
    "\n",
    "# Function to tokenize a sentence and return its length\n",
    "def tokenize_sentence(sentence, tokenizer):\n",
    "    tokens = tokenizer.encode(sentence, max_length=1024, truncation=True)\n",
    "    return len(tokens)\n",
    "\n",
    "# Function to generate a csv byte offset file from the original dataset\n",
    "# used to work with Carlini code only\n",
    "def generate_byte_dataset(input_file, output_file, tokenizer):\n",
    "    print(\"Generating byte offset dataset from file: \", input_file)\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    if not os.path.exists(TARGET_DIR):\n",
    "        os.makedirs(TARGET_DIR)\n",
    "        \n",
    "    with open(output_file, \"w\", newline='', encoding=\"utf-8\") as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        #csv_writer.writerow([\"exid\", \"fid\", \"line_byte_offset\", \"start\", \"end\", \"take_before\", \"take_after\", \"internal_offset\", \"size\", \"start_byte\", \"end_byte\", \"count\"])\n",
    "        csv_writer.writerow([\"exid\", \"size\"])\n",
    "        \n",
    "        exid = 1 # start at 1\n",
    "\n",
    "        #line_byte_offset = 0\n",
    "        #fid = 0\n",
    "        for line in lines:\n",
    "            # Remove leading/trailing whitespaces and newline characters\n",
    "            line = line.strip()\n",
    "\n",
    "                # Calculate the end position (end of sentence)\n",
    "                #end = len(line) - 1\n",
    "\n",
    "                # Tokenize the sentence and get its length\n",
    "            size = len(tokenizer.encode(line, truncation=True))\n",
    "                \n",
    "                # Write the row to the CSV file\n",
    "                #csv_writer.writerow([exid, fid, line_byte_offset, 0, end, 0, 0, 0, size, -1, -1, -1])\n",
    "            csv_writer.writerow([exid, size])\n",
    "                # Update line byte offset for the next sentence\n",
    "                #line_byte_offset += len(line) + 1  # Add 1 for the newline character\n",
    "            \n",
    "            exid += 1  # Always increment the example ID to keep in sync with original dataset\n",
    "\n",
    "\n",
    "# Function to generate a jsonlines version of dataset\n",
    "# input here is a text file\n",
    "def text_to_jsonlines(input_file, output_file):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f_input, \\\n",
    "         open(output_file, \"w\", encoding=\"utf-8\") as f_output:\n",
    "        id = 1\n",
    "\n",
    "        for line in f_input:\n",
    "            # Remove leading/trailing whitespaces and newline characters\n",
    "            line = line.strip()\n",
    "\n",
    "            \n",
    "            # Create a JSON object with a \"text\" field containing the line\n",
    "            json_object = {\"exid\": id,\n",
    "                           \"text\": line}\n",
    "            \n",
    "            # Write the JSON object to the output file as a single line\n",
    "            json.dump(json_object, f_output, ensure_ascii=False)\n",
    "            f_output.write('\\n')\n",
    "            id += 1\n",
    "\n",
    "# Function to generate a jsonlines version of dataset\n",
    "# input here is a numpy array of tokenized data (using token IDs)\n",
    "def generations_to_jsonl(output_file_path: str, data: np.ndarray):\n",
    "    \"\"\"Converts the tokenized data to a JSONL file at `path`.\"\"\"\n",
    "\n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\", newline='') as file:\n",
    "        id = 0\n",
    "        for row in data:\n",
    "            # Convert token IDs to strings\n",
    "            # replace token space character with empty string\n",
    "            decoded_string = tokenizer.decode(row, skip_special_tokens=True).replace('Ġ', '')\n",
    "            line = decoded_string.strip()\n",
    "\n",
    "            # Skip empty lines\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            # Create a JSON object with a \"text\" field containing the line\n",
    "            json_object = {\"exid\": id,\n",
    "                           \"text\": line}\n",
    "\n",
    "            # Write the JSON object to the output file as a single line\n",
    "            json.dump(json_object, file, ensure_ascii=False)\n",
    "            file.write(\"\\n\")\n",
    "            id += 1\n",
    "\n",
    "    print(\"Decoded strings saved to: %s\", str(output_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operating on dataset base: EMEA/EMEA in language nl\n",
      "Dataset file: EMEA/EMEA.nl\n",
      "Byte offset base: ./datasets/EMEA/csv/EMEA.nl.csv\n"
     ]
    }
   ],
   "source": [
    "# 1. read data_config.json\n",
    "\n",
    "with open(\"data_config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "    dataset_base = os.path.join(config[\"dataset_dir\"], config[\"dataset_name\"])\n",
    "    dataset_file = os.path.join(dataset_base + \".\" + config[\"language\"])\n",
    "    \n",
    "print(\"Operating on dataset base:\", dataset_base, \"in language\", config[\"language\"])\n",
    "print(\"Dataset file:\", dataset_file)\n",
    "print(\"Byte offset base:\", byte_offset_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating byte offset dataset from file:  EMEA/EMEA.en\n"
     ]
    }
   ],
   "source": [
    "# 2. Generate a byte offset version of the dataset for inspection purposes\n",
    "generate_byte_dataset(dataset_file, byte_offset_base, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17156"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_large_entries(byte_offset_base, TOKEN_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered rows have been written to ./datasets/ECB/csv/extracted/ecb_en.txt-100.nl.csv\n"
     ]
    }
   ],
   "source": [
    "def filter_csv(input_file, output_file, min_size):\n",
    "    # Open the input CSV file for reading\n",
    "    with open(input_file, mode='r', newline='', encoding='utf-8') as infile:\n",
    "        # Create a CSV reader object\n",
    "        reader = csv.DictReader(infile)\n",
    "        \n",
    "        # Open the output CSV file for writing\n",
    "        with open(output_file, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "            # Create a CSV writer object\n",
    "            writer = csv.DictWriter(outfile, fieldnames=reader.fieldnames)\n",
    "            \n",
    "            # Write the header to the output file\n",
    "            writer.writeheader()\n",
    "            \n",
    "            # Iterate through each row in the input file\n",
    "            for row in reader:\n",
    "                # Check if the size column value is at least min_size\n",
    "                if int(row['size']) >= min_size:\n",
    "                    # Write the row to the output file\n",
    "                    writer.writerow(row)\n",
    "\n",
    "# Input and output file paths\n",
    "input_csv = os.path.join(TARGET_DIR, BYTE_OFFSET_FILE)\n",
    "output_csv = os.path.join(TARGET_DIR, DATASET_NAME + \"-\" + str(TOKEN_LENGTH) + \".\" + LANGUAGE + \".csv\")\n",
    "\n",
    "# Call the function to filter the CSV file\n",
    "filter_csv(\"datasets/ECB/csv/ECB.nl.csv\", \"datasets/ECB/csv/ECB-100.nl.csv\", TOKEN_LENGTH)\n",
    "\n",
    "print(f\"Filtered rows have been written to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2440\n",
      "19619\n",
      "698\n",
      "Common exids have been written to datasets/ECB/csv/common_exids-100.csv\n"
     ]
    }
   ],
   "source": [
    "def read_exids_from_csv(file):\n",
    "    # integer set\n",
    "    exids = set()\n",
    "    with open(file, mode='r', newline='', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        next(reader, None)  # Skip the header\n",
    "        for row in reader:\n",
    "            exids.add(int(row['exid']))\n",
    "    return exids, len(exids)\n",
    "\n",
    "def find_common_exids(file1, file2):\n",
    "    exids1, len1 = read_exids_from_csv(file1)\n",
    "    print(len1)\n",
    "    exids2, len2 = read_exids_from_csv(file2)\n",
    "    print(len2)\n",
    "    common_exids = exids1.intersection(exids2)\n",
    "    # sort\n",
    "    common_exids = sorted(common_exids)\n",
    "\n",
    "    print(len(common_exids))\n",
    "    return common_exids\n",
    "\n",
    "def write_exids_to_file(exids, output_file):\n",
    "    with open(output_file, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        for exid in exids:\n",
    "            writer.writerow([exid])\n",
    "\n",
    "# Input CSV file paths\n",
    "csv_file1 = 'datasets/ECB/csv/ECB-100.en.csv'\n",
    "csv_file2 = 'datasets/ECB/csv/ECB-100.nl.csv'\n",
    "output_csv = 'datasets/ECB/csv/common_exids-100.csv'\n",
    "\n",
    "# Find common exids and write them to the output file\n",
    "common_exids = find_common_exids(csv_file1, csv_file2)\n",
    "write_exids_to_file(common_exids, output_csv)\n",
    "\n",
    "print(f\"Common exids have been written to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7398\n",
      "['81', '83', '568', '577', '765', '766', '1061', '1289', '1396', '1876']\n",
      "Truncating sentences in file:  nl-en/europarl-v7.nl-en.nl.jsonl  to  100  tokens\n",
      "Truncated  7398  sentences to  nl-en/europarl-v7.nl-en-100.nl.jsonl\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "def read_common_exids(file):\n",
    "    exids = []\n",
    "    with open(file, mode='r', newline='', encoding='utf-8') as infile:\n",
    "        reader = csv.reader(infile)\n",
    "        for row in reader:\n",
    "            exid = row  # Strip any leading/trailing whitespace\n",
    "            exid = exid[0]\n",
    "            exids.append(exid)\n",
    "    return exids\n",
    "\n",
    "def trunc_json(input_file, output_file, max_tokens, exid_list):\n",
    "    # takes common example ids from csv file and truncates the corresponding sentences in the jsonl file\n",
    "    # produces a new jsonl file with the truncated sentences to length max_tokens\n",
    "    print(\"Truncating sentences in file: \", input_file, \" to \", max_tokens, \" tokens\")\n",
    "    count = 0\n",
    "    \n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f_input, \\\n",
    "         open(output_file, \"w\", encoding=\"utf-8\") as f_output:\n",
    "        \n",
    "        # loop over all examples in the original dataset (jsonl version)\n",
    "        for line in f_input:\n",
    "            # Remove leading/trailing whitespaces and newline characters\n",
    "            json_object = json.loads(line)\n",
    "            \n",
    "            exid = json_object[\"exid\"]\n",
    "            \n",
    "\n",
    "            if(str(exid) not in exid_list):\n",
    "                continue\n",
    "\n",
    "            else: \n",
    "                sentence = json_object[\"text\"]\n",
    "                # Tokenize the sentence\n",
    "                tokens = tokenizer.tokenize(sentence)\n",
    "            \n",
    "                # Truncate the tokenized sentece to max amount of tokens\n",
    "                truncated_sentence = truncate_tokens(tokens, max_tokens)\n",
    "\n",
    "                trunc_obj = {\"exid\": exid,\n",
    "                             \"text\": truncated_sentence}\n",
    "                    \n",
    "                # Write the truncated sentence to the output file\n",
    "                json.dump(trunc_obj, f_output, ensure_ascii=False)\n",
    "                f_output.write('\\n')\n",
    "                count += 1\n",
    "    print(\"Truncated \", count, \" sentences to \", output_file)\n",
    "    print(\"Done!\")\n",
    "\n",
    "exid_list = read_common_exids('datasets/csv/common_exids-100.csv')\n",
    "print(len(exid_list))\n",
    "\n",
    "input_file = 'nl-en/europarl-v7.nl-en.nl.jsonl'\n",
    "output_file = 'nl-en/europarl-v7.nl-en-100.nl.jsonl'\n",
    "\n",
    "trunc_json(input_file, output_file, TOKEN_LENGTH, exid_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONL file nl-en/europarl-v7.nl-en.en.jsonl already exists, skipping generation\n"
     ]
    }
   ],
   "source": [
    "# Generate a jsonlines version of the dataset\n",
    "jsonlines_base = os.path.join(DATASET_DIR, DATASET_NAME + \".\" + LANGUAGE + \".jsonl\")\n",
    "\n",
    "# check if file exists and has content\n",
    "if os.path.exists(jsonlines_base) and os.path.getsize(jsonlines_base) > 0:\n",
    "    print(\"JSONL file\", jsonlines_base, \"already exists, skipping generation\")\n",
    "else: \n",
    "    text_to_jsonlines(dataset_file, jsonlines_base)\n",
    "    print(\"JSONL file saved to: \", jsonlines_base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1846208603.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[16], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    for line in lines:\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "def process_train_data(byte_offset_csv, output_file, max_tokens):\n",
    "    # Read the byte offset CSV file\n",
    "    with open(byte_offset_csv, \"r\", newline='', encoding=\"utf-8\") as csvfile:\n",
    "        with open(output_file, \"w\", newline='', encoding=\"utf-8\") as f_output:\n",
    "            csv_reader = csv.DictReader(csvfile)\n",
    "            rows = len(list(csv_reader))\n",
    "                \n",
    "            # Iterate through each row in the CSV file\n",
    "            for i in range(rows): \n",
    "                ids = []\n",
    "                exid = int(rows[i][\"exid\"])\n",
    "                size = int(rows[i][\"size\"])\n",
    "\n",
    "                # sentence is long enough\n",
    "                if (size >= max_tokens):\n",
    "                    ids.append(exid)\n",
    "                    json_object = {\"exid\": ids,\n",
    "                                   \"size\": size}\n",
    "                    json.dump(json_object, f_output)\n",
    "                    f_output.write(\"\\n\")\n",
    "                \n",
    "                elif(size < max_tokens):\n",
    "                    if(rows[i+1][\"size\"] < max_tokens):\n",
    "                        ids.append(exid)\n",
    "                        ids.append(int(rows[i+1][\"exid\"]))\n",
    "                        size = size + int(rows[i+1][\"size\"])\n",
    "\n",
    "                        json_object = {\"exid\": ids,\n",
    "                                       \"size\": size}\n",
    "                        json.dump(json_object, f_output)\n",
    "                        f_output.write(\"\\n\")\n",
    "                        i +=1\n",
    "                    else: \n",
    "                        i+=1      \n",
    "                    \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
