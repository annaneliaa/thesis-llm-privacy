{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from transformers import GPT2Tokenizer\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset constants\n",
    "\n",
    "# path to the (pretraining) dataset of the model\n",
    "DATASET_DIR = \"DGT-TM\"\n",
    "# file name of text version of the dataset\n",
    "DATASET_NAME = \"vol-2015\"\n",
    "# language of the setup\n",
    "LANGUAGE = \"en\"\n",
    "# target directory for the csv files\n",
    "SOURCE_DIR = \"./datasets\"\n",
    "# desired token length of examples\n",
    "EXAMPLE_TOKEN_LEN = 100\n",
    "# target file name for the byte off set csv files\n",
    "BYTE_OFFSET_FILE = DATASET_NAME + \".\" + LANGUAGE + \".csv\"\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "byte_offset_base = os.path.join(SOURCE_DIR, DATASET_DIR, \"csv\", BYTE_OFFSET_FILE)\n",
    "\n",
    "# create data_config.json from constants\n",
    "config = {\n",
    "    \"dataset_dir\": DATASET_DIR,\n",
    "    \"dataset_name\": DATASET_NAME,\n",
    "    \"language\": LANGUAGE,\n",
    "    \"example_token_len\": EXAMPLE_TOKEN_LEN,\n",
    "    \"source_dir\": SOURCE_DIR,\n",
    "    \"byte_offset_file\": BYTE_OFFSET_FILE,\n",
    "}\n",
    "\n",
    "with open(\"data_config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data set inspection functions\n",
    "\n",
    "# function to count the longest token sequence in a dataset file \n",
    "def max_tokens_in_sentence(file_path):\n",
    "    max_tokens = 0\n",
    "\n",
    "    print(\"Counting max tokens in file: \", file_path)\n",
    "    print(\"This may take a while...\")\n",
    "    with open(file_path, 'r') as file:\n",
    "        i = 0\n",
    "        for line in file:\n",
    "            i +=1\n",
    "            if(line):\n",
    "            # Tokenize the sentence\n",
    "                tokens = nltk.word_tokenize(line)\n",
    "                num_tokens = len(tokens)\n",
    "                # Update max_tokens if current sentence has more tokens\n",
    "                if num_tokens > max_tokens:\n",
    "                    max_tokens = num_tokens\n",
    "    \n",
    "        print(\"Max tokens in file: \", max_tokens)\n",
    "    return max_tokens\n",
    "\n",
    "# function to count the number of examples in a dataset file with more tokens than a given threshold\n",
    "def count_large_entries(csv_file, tokens):\n",
    "    # Open the CSV file for reading\n",
    "    with open(csv_file, \"r\", newline='', encoding=\"utf-8\") as csvfile:\n",
    "        csv_reader = csv.DictReader(csvfile)\n",
    "        \n",
    "        # Initialize a counter for large entries\n",
    "        large_entry_count = 0\n",
    "        \n",
    "        # Iterate through each row in the CSV file\n",
    "        for row in csv_reader:\n",
    "            # Convert the value in the \"size\" column to an integer\n",
    "            # this is the number of tokens in the example\n",
    "            size = int(row[\"size\"])\n",
    "            \n",
    "            # Check if the size is greater than or equal to the amount of tokens supplied\n",
    "            if size >= tokens:\n",
    "                # Increment the counter if the condition is met\n",
    "                large_entry_count += 1\n",
    "                \n",
    "    return large_entry_count\n",
    "\n",
    "# dataset generation functions\n",
    "def truncate_sentence(sentence, max_tokens):\n",
    "    # Tokenize the sentence\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    \n",
    "    # Truncate to max_tokens tokens\n",
    "    truncated_tokens = tokens[:max_tokens]\n",
    "    \n",
    "    # Convert tokens back to string\n",
    "    truncated_sentence = tokenizer.convert_tokens_to_string(truncated_tokens)\n",
    "    \n",
    "    return truncated_sentence\n",
    "\n",
    "def truncate_tokens(tokens, max_tokens):\n",
    "    # Truncate to max_tokens tokens\n",
    "    truncated_tokens = tokens[:max_tokens]\n",
    "    \n",
    "    # Convert tokens back to string\n",
    "    truncated_sentence = tokenizer.convert_tokens_to_string(truncated_tokens)\n",
    "    \n",
    "    return truncated_sentence\n",
    "\n",
    "def filter_truncate_json_sentences(input_file, output_file, max_tokens):\n",
    "    print(\"Filtering and truncating sentences in file: \", input_file, \" to \", max_tokens, \" tokens\")\n",
    "    \n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f_input, \\\n",
    "         open(output_file, \"w\", encoding=\"utf-8\") as f_output:\n",
    "        \n",
    "        for line in f_input:\n",
    "            json_object = json.loads(line)\n",
    "\n",
    "            sentence = json_object[\"text\"]\n",
    "\n",
    "            # Skip empty lines\n",
    "            if not sentence:\n",
    "                continue\n",
    "\n",
    "            exid = json_object[\"exid\"]\n",
    "            # Remove leading/trailing whitespaces and newline characters\n",
    "            sentence = sentence.strip()\n",
    "            \n",
    "            # Tokenize the sentence\n",
    "            tokens = tokenizer.tokenize(sentence)\n",
    "            \n",
    "            # Check if the number of tokens exceeds the maximum\n",
    "            if len(tokens) >= max_tokens:\n",
    "\n",
    "                # Truncate the tokenized sentece to max amount of tokens\n",
    "                truncated_sentence = truncate_tokens(tokens, max_tokens)\n",
    "\n",
    "                # Create a JSON object with a \"text\" field containing the line\n",
    "                # and the original example ID\n",
    "                trunc_object = {\"exid\": exid,\n",
    "                               \"text\": truncated_sentence}\n",
    "                \n",
    "                # Write the JSON object to the output file as a single line\n",
    "                json.dump(trunc_object, f_output, ensure_ascii=False)\n",
    "                f_output.write('\\n')\n",
    "\n",
    "def filter_and_truncate_sentences(input_file, output_file, max_tokens):\n",
    "    print(\"Filtering and truncating sentences in file: \", input_file, \" to \", max_tokens, \" tokens\")\n",
    "    \n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f_input, \\\n",
    "         open(output_file, \"w\", encoding=\"utf-8\") as f_output:\n",
    "        \n",
    "        for line in f_input:\n",
    "            # Remove leading/trailing whitespaces and newline characters\n",
    "            sentence = line.strip()\n",
    "            \n",
    "            # Tokenize the sentence\n",
    "            tokens = tokenizer.tokenize(sentence)\n",
    "            \n",
    "            # Check if the number of tokens exceeds the maximum\n",
    "            if len(tokens) >= max_tokens:\n",
    "\n",
    "                # Truncate the tokenized sentece to max amount of tokens\n",
    "                truncated_sentence = truncate_tokens(tokens, max_tokens)\n",
    "                \n",
    "                # Write the truncated sentence to the output file\n",
    "                f_output.write(truncated_sentence + \"\\n\")\n",
    "                \n",
    "# Function to generate a csv byte offset file from the original dataset\n",
    "# used to work with Carlini code only\n",
    "def generate_byte_dataset(input_file, output_file, tokenizer):\n",
    "    print(\"Generating byte offset dataset from file: \", input_file)\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    if not os.path.exists(SOURCE_DIR):\n",
    "        os.makedirs(SOURCE_DIR)\n",
    "        \n",
    "    with open(output_file, \"w\", newline='', encoding=\"utf-8\") as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        #csv_writer.writerow([\"exid\", \"fid\", \"line_byte_offset\", \"start\", \"end\", \"take_before\", \"take_after\", \"internal_offset\", \"size\", \"start_byte\", \"end_byte\", \"count\"])\n",
    "        csv_writer.writerow([\"exid\", \"size\"])\n",
    "        \n",
    "        exid = 1 # start at 1\n",
    "\n",
    "        #line_byte_offset = 0\n",
    "        #fid = 0\n",
    "        for line in lines:\n",
    "            # Remove leading/trailing whitespaces and newline characters\n",
    "            line = line.strip()\n",
    "\n",
    "                # Calculate the end position (end of sentence)\n",
    "                #end = len(line) - 1\n",
    "\n",
    "                # Tokenize the sentence and get its length\n",
    "            size = len(tokenizer.encode(line, truncation=True))\n",
    "                \n",
    "            # Write the row to the CSV file\n",
    "            csv_writer.writerow([exid, size])\n",
    "            \n",
    "            exid += 1  # Always increment the example ID to keep in sync with original dataset\n",
    "\n",
    "# Function to generate a csv byte offset file from the original dataset\n",
    "def generate_byte_dataset_jsonl(input_file, output_file, tokenizer):\n",
    "    print(\"Generating byte offset dataset from file: \", input_file)\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    if not os.path.exists(SOURCE_DIR):\n",
    "        os.makedirs(SOURCE_DIR)\n",
    "        \n",
    "    with open(output_file, \"w\", newline='', encoding=\"utf-8\") as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow([\"exid\", \"size\"])\n",
    "\n",
    "        for line in lines:\n",
    "            # Tokenize the sentence and get its length\n",
    "            json_object = json.loads(line)\n",
    "            exid = json_object[\"exid\"]\n",
    "            sentence = json_object[\"text\"]\n",
    "            size = len(tokenizer.encode(sentence, truncation=True))\n",
    "                \n",
    "            # Write the row to the CSV file\n",
    "            csv_writer.writerow([exid, size])\n",
    "                # Update line byte offset for the next sentence\n",
    "                #line_byte_offset += len(line) + 1  # Add 1 for the newline character\n",
    "\n",
    "# Function to generate a jsonlines version of dataset\n",
    "# input here is a text file\n",
    "def text_to_jsonlines(input_file, output_file):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f_input, \\\n",
    "         open(output_file, \"w\", encoding=\"utf-8\") as f_output:\n",
    "        id = 1\n",
    "\n",
    "        for line in f_input:\n",
    "            # Remove leading/trailing whitespaces and newline characters\n",
    "            line = line.strip()\n",
    "\n",
    "            \n",
    "            # Create a JSON object with a \"text\" field containing the line\n",
    "            json_object = {\"exid\": id,\n",
    "                           \"text\": line}\n",
    "            \n",
    "            # Write the JSON object to the output file as a single line\n",
    "            json.dump(json_object, f_output, ensure_ascii=False)\n",
    "            f_output.write('\\n')\n",
    "            id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operating on dataset base: EMEA/EMEA-c in language en\n",
      "Dataset file: EMEA/EMEA-c.en\n",
      "Byte offset base: ./datasets/EMEA/csv/EMEA-c.en.csv\n"
     ]
    }
   ],
   "source": [
    "# 1. read data_config.json\n",
    "\n",
    "with open(\"data_config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "    dataset_base = os.path.join(config[\"dataset_dir\"], config[\"dataset_name\"])\n",
    "    dataset_file = os.path.join(dataset_base + \".\" + config[\"language\"])\n",
    "    \n",
    "print(\"Operating on dataset base:\", dataset_base, \"in language\", config[\"language\"])\n",
    "print(\"Dataset file:\", dataset_file)\n",
    "print(\"Byte offset base:\", byte_offset_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating byte offset dataset from file:  ECB/ECB.en\n"
     ]
    }
   ],
   "source": [
    "# 2. Generate a byte offset version of the dataset for inspection purposes\n",
    "# from text .en or .nl file\n",
    "generate_byte_dataset(dataset_file, byte_offset_base, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating byte offset dataset from file:  EMEA/EMEA-c.nl.jsonl\n"
     ]
    }
   ],
   "source": [
    "in_file = dataset_file + \".jsonl\"\n",
    "# from .jsonl version of dataset\n",
    "generate_byte_dataset_jsonl(in_file, byte_offset_base, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48134"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_large_entries(byte_offset_base, EXAMPLE_TOKEN_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered rows have been written to ./datasets/EMEA/csv/EMEA-c-100.nl.csv\n"
     ]
    }
   ],
   "source": [
    "# This function filters the CSV file based on the size column\n",
    "def filter_csv(input_file, output_file, min_size):\n",
    "    # Open the input CSV file for reading\n",
    "    with open(input_file, mode='r', newline='', encoding='utf-8') as infile:\n",
    "        # Create a CSV reader object\n",
    "        reader = csv.DictReader(infile)\n",
    "        \n",
    "        # Open the output CSV file for writing\n",
    "        with open(output_file, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "            # Create a CSV writer object\n",
    "            writer = csv.DictWriter(outfile, fieldnames=reader.fieldnames)\n",
    "            \n",
    "            # Write the header to the output file\n",
    "            writer.writeheader()\n",
    "            \n",
    "            # Iterate through each row in the input file\n",
    "            for row in reader:\n",
    "                # Check if the size column value is at least min_size\n",
    "                if int(row['size']) >= min_size:\n",
    "                    # Write the row to the output file\n",
    "                    writer.writerow(row)\n",
    "\n",
    "# Input and output file paths\n",
    "input_csv = os.path.join(byte_offset_base)\n",
    "output_csv = os.path.join(SOURCE_DIR, DATASET_DIR, \"csv\", DATASET_NAME + \"-\" + str(EXAMPLE_TOKEN_LEN) + \".\" + LANGUAGE + \".csv\")\n",
    "\n",
    "# Call the function to filter the CSV file\n",
    "filter_csv(input_csv, output_csv, EXAMPLE_TOKEN_LENGTH)\n",
    "\n",
    "print(f\"Filtered rows have been written to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9170\n",
      "48133\n",
      "8309\n",
      "Common exids have been written to ./datasets/EMEA/csv/common_exids-100.csv\n"
     ]
    }
   ],
   "source": [
    "def read_exids_from_csv(file):\n",
    "    # integer set\n",
    "    exids = set()\n",
    "    with open(file, mode='r', newline='', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        for row in reader:\n",
    "            exids.add(int(row['exid']))\n",
    "    return exids, len(exids)\n",
    "\n",
    "def find_common_exids(file1, file2):\n",
    "    exids1, len1 = read_exids_from_csv(file1)\n",
    "    print(len1)\n",
    "    exids2, len2 = read_exids_from_csv(file2)\n",
    "    print(len2)\n",
    "    common_exids = exids1.intersection(exids2)\n",
    "    # sort\n",
    "    common_exids = sorted(common_exids)\n",
    "\n",
    "    print(len(common_exids))\n",
    "    return common_exids\n",
    "\n",
    "def write_exids_to_file(exids, output_file):\n",
    "    with open(output_file, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        for exid in exids:\n",
    "            writer.writerow([exid])\n",
    "\n",
    "# Input CSV file paths\n",
    "csv_file1 = os.path.join(SOURCE_DIR, DATASET_DIR, \"csv\", DATASET_NAME + \"-\" + str(EXAMPLE_TOKEN_LEN) + \".\" + \"en\" + \".csv\")\n",
    "csv_file2 = os.path.join(SOURCE_DIR, DATASET_DIR, \"csv\", DATASET_NAME + \"-\" + str(EXAMPLE_TOKEN_LEN) + \".\" + \"nl\" + \".csv\")\n",
    "output_csv = os.path.join(SOURCE_DIR, DATASET_DIR, \"csv\", \"common_exids-\" + str(EXAMPLE_TOKEN_LEN) + \".csv\")\n",
    "\n",
    "# Find common exids and write them to the output file\n",
    "common_exids = find_common_exids(csv_file1, csv_file2)\n",
    "write_exids_to_file(common_exids, output_csv)\n",
    "\n",
    "print(f\"Common exids have been written to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8309 common exids found\n",
      "Truncating sentences in file:  EMEA/EMEA-c.en.jsonl  to  100  tokens\n",
      "Truncated  8309  sentences to  EMEA/EMEA-c-100.en.jsonl\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "def read_common_exids(file):\n",
    "    exids = []\n",
    "    with open(file, mode='r', newline='', encoding='utf-8') as infile:\n",
    "        reader = csv.reader(infile)\n",
    "        for row in reader:\n",
    "            exid = row  # Strip any leading/trailing whitespace\n",
    "            exid = exid[0]\n",
    "            exids.append(exid)\n",
    "    return exids\n",
    "\n",
    "def trunc_json(input_file, output_file, max_tokens, exid_list):\n",
    "    # takes common example ids from csv file and truncates the corresponding sentences in the jsonl file\n",
    "    # produces a new jsonl file with the truncated sentences to length max_tokens\n",
    "    print(\"Truncating sentences in file: \", input_file, \" to \", max_tokens, \" tokens\")\n",
    "    count = 0\n",
    "    \n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f_input, \\\n",
    "         open(output_file, \"w\", encoding=\"utf-8\") as f_output:\n",
    "        \n",
    "        # loop over all examples in the original dataset (jsonl version)\n",
    "        for line in f_input:\n",
    "            # Remove leading/trailing whitespaces and newline characters\n",
    "            json_object = json.loads(line)\n",
    "            \n",
    "            exid = json_object[\"exid\"]\n",
    "            \n",
    "\n",
    "            if(str(exid) not in exid_list):\n",
    "                continue\n",
    "\n",
    "            else: \n",
    "                sentence = json_object[\"text\"]\n",
    "                # Tokenize the sentence\n",
    "                tokens = tokenizer.tokenize(sentence)\n",
    "            \n",
    "                # Truncate the tokenized sentece to max amount of tokens\n",
    "                truncated_sentence = truncate_tokens(tokens, max_tokens)\n",
    "\n",
    "                trunc_obj = {\"exid\": exid,\n",
    "                             \"text\": truncated_sentence}\n",
    "                    \n",
    "                # Write the truncated sentence to the output file\n",
    "                json.dump(trunc_obj, f_output, ensure_ascii=False)\n",
    "                f_output.write('\\n')\n",
    "                count += 1\n",
    "    print(\"Truncated \", count, \" sentences to \", output_file)\n",
    "    print(\"Done!\")\n",
    "\n",
    "# read common exids\n",
    "path = os.path.join(SOURCE_DIR, DATASET_DIR, \"csv\", \"common_exids-\" + str(EXAMPLE_TOKEN_LEN) + \".csv\")\n",
    "exid_list = read_common_exids(path)\n",
    "print(len(exid_list), \"common exids found\")\n",
    "\n",
    "input_file = os.path.join(DATASET_DIR, DATASET_NAME + \".\" + LANGUAGE + \".jsonl\")\n",
    "output_file = os.path.join(DATASET_DIR, DATASET_NAME + \"-\" + str(EXAMPLE_TOKEN_LEN) + \".\" + LANGUAGE + \".jsonl\")\n",
    "\n",
    "trunc_json(input_file, output_file, EXAMPLE_TOKEN_LEN, exid_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to concat sentences in dataset to get more examples with >= desired token length\n",
    "# uses the byte offset csv file for faster processing\n",
    "def process_train_data(byte_offset_csv, output_file, max_tokens):\n",
    "    # Read the byte offset CSV file\n",
    "    with open(byte_offset_csv, \"r\", newline='', encoding=\"utf-8\") as csvfile:\n",
    "        with open(output_file, \"w\", newline='', encoding=\"utf-8\") as out_file:\n",
    "            csv_reader = list(csv.DictReader(csvfile))\n",
    "            idx = 0\n",
    "            while idx < len(csv_reader):\n",
    "                ids = []\n",
    "                exid = int(csv_reader[idx][\"exid\"])\n",
    "                size = int(csv_reader[idx][\"size\"])\n",
    "\n",
    "                # If the current line has enough tokens, process it\n",
    "                if size >= max_tokens:\n",
    "                    ids.append(exid)\n",
    "                    json_object = {\"exid\": ids, \"size\": size}\n",
    "                    json.dump(json_object, out_file)\n",
    "                    out_file.write(\"\\n\")\n",
    "                    idx += 1\n",
    "                else:\n",
    "                    # If the next line also does not have enough tokens, concatenate them\n",
    "                    if idx + 1 < len(csv_reader) and int(csv_reader[idx + 1][\"size\"]) < max_tokens:\n",
    "                        ids.append(exid)\n",
    "                        ids.append(int(csv_reader[idx + 1][\"exid\"]))\n",
    "                        size += int(csv_reader[idx + 1][\"size\"])\n",
    "                        json_object = {\"exid\": ids, \"size\": size}\n",
    "                        json.dump(json_object, out_file)\n",
    "                        out_file.write(\"\\n\")\n",
    "                        idx += 2\n",
    "                    else:\n",
    "                        # Skip to the next line\n",
    "                        idx += 1\n",
    "\n",
    "def count_large_entries_json(json_file, max_tokens):\n",
    "    # Open the JSON file for reading\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        # Initialize a counter for large entries\n",
    "        large_entry_count = 0\n",
    "        \n",
    "        # Iterate through each line in the JSON file\n",
    "        for line in file:\n",
    "            # Parse the JSON object from the line\n",
    "            entry = json.loads(line)\n",
    "            \n",
    "            # Extract the size value from the JSON object\n",
    "            size = entry[\"size\"]\n",
    "            \n",
    "            # Check if the size is greater than or equal to the max_tokens\n",
    "            if size >= max_tokens:\n",
    "                # Increment the counter if the condition is met\n",
    "                large_entry_count += 1\n",
    "                \n",
    "    return large_entry_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = os.path.join(SOURCE_DIR, DATASET_DIR, DATASET_NAME + \".\" + LANGUAGE + \".jsonl\")\n",
    "process_train_data(byte_offset_base, out_file, EXAMPLE_TOKEN_LEN)\n",
    "\n",
    "# NOTE: you should only need to run process_train_data once for EN, so that you have 1 unique versioin of exids groups!!!!!\n",
    "# NOTE: should we keep repeating this if we want to test with 200 tokens for extra context?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9210"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_large_entries_json(out_file, EXAMPLE_TOKEN_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_dataset(json_file, dataset_file, output_file):\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as in_file, open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        # Initialize new exid counter\n",
    "        new_exid = 1\n",
    "\n",
    "        # Read the dataset file into a list of lines\n",
    "        with open(dataset_file, 'r') as file:\n",
    "            # this list will hold the dataset, starting at index 0\n",
    "            dataset = file.readlines()\n",
    "        \n",
    "        # Get the total number of lines in the dataset\n",
    "        total_lines = len(dataset)\n",
    "        \n",
    "        # Read the input JSON file line by line\n",
    "        lines = in_file.readlines()\n",
    "        for line in lines:\n",
    "            # Parse the JSON object\n",
    "            data = json.loads(line)\n",
    "            exids = data[\"exid\"]\n",
    "            \n",
    "            concat_sentence = \"\"\n",
    "            for exid in exids:\n",
    "                concat_sentence += dataset[exid-1].strip() + \" \"\n",
    "            \n",
    "            # Remove trailing space\n",
    "            concat_sentence = concat_sentence.strip()\n",
    "            \n",
    "            new_data = {\n",
    "                \"exid\": new_exid,\n",
    "                \"text\": concat_sentence\n",
    "            }\n",
    "\n",
    "            new_exid += 1  # Increment the exid for the next line\n",
    "            \n",
    "            # Write the new JSON object to the output file\n",
    "            json.dump(new_data, outfile, ensure_ascii=False)\n",
    "            outfile.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMEA/ H/ C/ 471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NOTE: reformat nl dataset with ENGLISH paired json faile!!\n",
    "reformat_dataset(\"datasets/EMEA/EMEA.en.jsonl\", \"EMEA/EMEA.en\", \"EMEA/EMEA-c.en.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training wants plain text, not npy so we converting concat trunc json version to plain text again\n",
    "def extract_text_from_json(json_file, output_txt_file):\n",
    "    with open(json_file, 'r', encoding='utf-8') as infile, open(output_txt_file, 'w', encoding='utf-8') as outfile:\n",
    "        for line in infile:\n",
    "            data = json.loads(line)\n",
    "            text = data[\"text\"]\n",
    "            outfile.write(text + \"\\n\")\n",
    "\n",
    "# Example usage\n",
    "json_file = 'EMEA/EMEA-c-100.nl.jsonl'  # Replace with your JSON file name\n",
    "output_txt_file = 'EMEA/EMEA-c-100.nl'  # The output text file\n",
    "extract_text_from_json(json_file, output_txt_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-06 21:31:49,438 - INFO - Parsing arguments...\n",
      "Parsing arguments...\n",
      "2024-06-06 21:31:49,438 - INFO - Loading tokenizer...\n",
      "Loading tokenizer...\n",
      "2024-06-06 21:31:49,661 - INFO - Counting tokens for en...\n",
      "Counting tokens for en...\n",
      "2024-06-06 21:31:49,661 - INFO - This may take a while depending on the size of the dataset...\n",
      "This may take a while depending on the size of the dataset...\n",
      "Generating byte offset dataset from file:  DGT-TM/vol-2015.en\n",
      "2024-06-06 21:32:30,012 - INFO - Number of samples >= 100 tokens in ./datasets/DGT-TM/csv/vol-2015.en.csv: 11316\n",
      "Number of samples >= 100 tokens in ./datasets/DGT-TM/csv/vol-2015.en.csv: 11316\n",
      "2024-06-06 21:32:30,013 - INFO - Filtering sentences for en...\n",
      "Filtering sentences for en...\n",
      "2024-06-06 21:32:30,510 - INFO - Generating JSONL for en...\n",
      "Generating JSONL for en...\n",
      "2024-06-06 21:32:34,163 - INFO - Counting tokens for nl...\n",
      "Counting tokens for nl...\n",
      "2024-06-06 21:32:34,163 - INFO - This may take a while depending on the size of the dataset...\n",
      "This may take a while depending on the size of the dataset...\n",
      "Generating byte offset dataset from file:  DGT-TM/vol-2015.nl\n",
      "2024-06-06 21:33:20,442 - INFO - Number of samples >= 100 tokens in ./datasets/DGT-TM/csv/vol-2015.nl.csv: 62180\n",
      "Number of samples >= 100 tokens in ./datasets/DGT-TM/csv/vol-2015.nl.csv: 62180\n",
      "2024-06-06 21:33:20,442 - INFO - Filtering sentences for nl...\n",
      "Filtering sentences for nl...\n",
      "2024-06-06 21:33:20,884 - INFO - Generating JSONL for nl...\n",
      "Generating JSONL for nl...\n",
      "Number of exids in file 1: %s 11316\n",
      "Number of exids in file 2: %s 62180\n",
      "Number of common exids found %s 1152\n",
      "2024-06-06 21:33:24,003 - INFO - Common exids have been written to ./datasets/DGT-TM/csv/common_exids-100.csv\n",
      "Common exids have been written to ./datasets/DGT-TM/csv/common_exids-100.csv\n",
      "2024-06-06 21:33:24,003 - INFO - 1152 common example IDs found\n",
      "1152 common example IDs found\n",
      "Truncating sentences in file:  DGT-TM/vol-2015.en.jsonl  to  100  tokens\n",
      "Truncated  1152  sentences to  DGT-TM/vol-2015-100.en.jsonl\n",
      "Done!\n",
      "Truncating sentences in file:  DGT-TM/vol-2015.nl.jsonl  to  100  tokens\n",
      "Truncated  1152  sentences to  DGT-TM/vol-2015-100.nl.jsonl\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "!python preprocessing.py --config_file data_config.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
